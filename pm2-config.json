{
  "gateway": {
    "name": "gateway",
    "command": "npm",
    "args": ["start"],
    "cwd": "gateway",
    "env": {
      "GATEWAY_MAX_CONCURRENT": "2",
      "GATEWAY_BUSY_MESSAGE": "Currently more than 2 users are active. Please try again later.",
      "LLM_UPSTREAMS": "http://127.0.0.1:5857,http://127.0.0.1:5957",
      "LLM_SERVER_MAX_CONCURRENT": "1",
      "GATEWAY_MAX_QUEUE": "10"
    }
  },
  "tunnel": {
    "name": "tunnel",
    "command": "cloudflared",
    "args": [
      "tunnel",
      "--config",
      "/Users/yapweijun/.cloudflared/config.yml",
      "run",
      "6da0b7da-ec47-41f5-904c-601a8d68748c"
    ],
    "cwd": "."
  },
  "llama": {
    "name": "llama",
    "command": "llama-server",
    "args": [
      "--model", "/Users/yapweijun/Library/Caches/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-Q4_K_M.gguf",
      "--port", "5857",
      "--ctx-size", "64000",
      "--threads", "4",
      "--n-gpu-layers", "8",
      "--parallel", "1",
      "--jinja"
    ],

    "cwd": "."
  },
  "llama2": {
    "name": "llama2",
    "command": "llama-server",
    "args": [
      "--model", "/Users/yapweijun/Library/Caches/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-Q4_K_M.gguf",
      "--port", "5957",
      "--ctx-size", "64000",
      "--threads", "4",
      "--n-gpu-layers", "8",
      "--parallel", "1",
      "--jinja"
    ],

    "cwd": "."
  }
}
