<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Home LLM + Cloudflare Tunnel Guide</title>
    <style>
      :root {
        color-scheme: light;
      }
      * {
        box-sizing: border-box;
      }
      body {
        margin: 0;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", system-ui, sans-serif;
        background: #f7f9fc;
        color: #0f172a;
        line-height: 1.6;
      }
      .page {
        max-width: 1100px;
        margin: 0 auto;
        padding: 32px 18px 60px;
      }
      header {
        margin-bottom: 28px;
      }
      .eyebrow {
        letter-spacing: 0.2em;
        text-transform: uppercase;
        color: #64748b;
        font-size: 12px;
        margin: 0 0 10px;
      }
      h1 {
        font-size: clamp(28px, 4vw, 40px);
        margin: 0 0 14px;
      }
      p.lead {
        margin: 0 0 14px;
        color: #334155;
      }
      .chips {
        display: flex;
        gap: 8px;
        flex-wrap: wrap;
        margin-top: 10px;
      }
      .chip {
        padding: 6px 10px;
        border: 1px solid #e2e8f0;
        border-radius: 999px;
        background: #fff;
        font-size: 13px;
        color: #334155;
      }
      section {
        margin-bottom: 26px;
      }
      .grid {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(230px, 1fr));
        gap: 12px;
      }
      .card {
        background: #fff;
        border: 1px solid #e2e8f0;
        border-radius: 12px;
        padding: 14px;
        box-shadow: 0 8px 24px rgba(15, 23, 42, 0.04);
      }
      .card h3,
      .card h2 {
        margin-top: 0;
        margin-bottom: 8px;
      }
      h2 {
        margin: 0 0 8px;
        font-size: 22px;
      }
      h3 {
        margin: 0 0 6px;
        font-size: 18px;
      }
      ul {
        padding-left: 18px;
        margin: 8px 0 0;
      }
      li {
        margin: 4px 0;
      }
      pre {
        background: #0f172a;
        color: #e2e8f0;
        padding: 14px;
        border-radius: 10px;
        overflow-x: auto;
        font-size: 13px;
        border: 1px solid #1e293b;
        margin: 12px 0 0;
      }
      code {
        font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono",
          "Courier New", monospace;
      }
      footer {
        border-top: 1px solid #e2e8f0;
        padding-top: 14px;
        margin-top: 32px;
        color: #64748b;
        font-size: 14px;
      }
      .note {
        color: #334155;
      }
      .tagline {
        font-size: 14px;
        color: #64748b;
      }
      .step-list {
        padding-left: 18px;
        margin: 4px 0 0;
        font-size: 14px;
      }
      .step-list li {
        margin: 2px 0;
      }
    </style>
  </head>
  <body>
    <div class="page">
      <header>
        <p class="eyebrow">Hands-on Guide</p>
        <h1>Host your own OpenAI-style API at home with llama.cpp + Cloudflare Tunnel</h1>
        <p class="lead">
          This article shows you, step by step, how to run a local GGUF model with
          <code>llama.cpp</code>, expose it safely through Cloudflare Tunnel, protect it with a Node.js API-key
          gateway, and call it from a simple HTML chat UI.
        </p>
        <p class="tagline">
          ç”¨æœ€ç®€å•çš„æ–¹å¼ï¼Œåœ¨å®¶é‡ŒæŠŠã€Œè‡ªå·±çš„ OpenAI APIã€æ¶èµ·æ¥ã€‚ä½ åªè¦ç…§ç€å¤åˆ¶è´´ä¸Šï¼Œä¸€æ­¥ä¸€æ­¥è·Ÿç€åšå°±å¯ä»¥ã€‚
        </p>
        <div class="chips">
          <span class="chip">llama.cpp</span>
          <span class="chip">Cloudflare Tunnel</span>
          <span class="chip">Node.js Gateway</span>
          <span class="chip">CORS & API Keys</span>
          <span class="chip">Chat UI</span>
        </div>
      </header>

      <!-- QUICK OVERVIEW -->
      <section class="card">
        <h2>0. Quick overviewï¼ˆæ‡’äººæ€»è§ˆï¼‰</h2>
        <p class="note">å¦‚æœä½ åªæƒ³çŸ¥é“æ•´ä½“åœ¨åšä»€ä¹ˆï¼Œå…ˆçœ‹è¿™æ®µã€‚</p>
        <ul class="step-list">
          <li><strong>Step 1ï¼š</strong>ç”¨ <code>llama-server</code> åœ¨æœ¬æœºè·‘ GGUF æ¨¡å‹ï¼ˆOpenAI é£æ ¼ APIï¼‰ã€‚</li>
          <li>
            <strong>Step 2ï¼š</strong>ç”¨ Node.js åšä¸€ä¸ª <em>API Gateway</em>ï¼š
            æ£€æŸ¥ <code>Authorization: Bearer sk-xxx</code>ï¼Œå¹¶å¤„ç† CORSã€‚
          </li>
          <li>
            <strong>Step 3ï¼š</strong>ç”¨ Cloudflare Tunnel æŠŠ
            <code>https://api.your-domain.com</code> å®‰å…¨åœ°æŒ‡åˆ°ä½ å®¶é‡Œçš„ Gatewayã€‚
          </li>
          <li>
            <strong>Step 4ï¼š</strong>ç”¨ä¸€ä¸ªç®€å•çš„ HTML <code>index.html</code> åš Chat UIï¼Œåœ¨æµè§ˆå™¨ç›´æ¥è°ƒç”¨ä½ çš„ APIã€‚
          </li>
        </ul>
        <pre><code>[Browser / Chat UI] -- HTTPS + Authorization: Bearer sk-xxx
        |
Cloudflare Edge (DNS / TLS / WAF)
        |
cloudflared tunnel
        |
Node.js Gateway @ localhost:8787  (API keys + CORS)
        |
llama-server (llama.cpp) @ localhost:5857  (GGUF model)</code></pre>
      </section>

      <!-- WHO / WHAT / TIME -->
      <section class="grid">
        <div class="card">
          <p class="eyebrow" style="letter-spacing: 0.08em">What you get</p>
          <p>
            An OpenAI-compatible <code>/v1/chat/completions</code> API running on your own hardware, protected by your
            own API keys and reachable from the internet.
          </p>
        </div>
        <div class="card">
          <p class="eyebrow" style="letter-spacing: 0.08em">Who this is for</p>
          <p>
            Junior developers who want a copy-paste friendly way to self-host an LLM with sensible defaults, without
            reverse-engineering docs all day.
          </p>
        </div>
        <div class="card">
          <p class="eyebrow" style="letter-spacing: 0.08em">Time to complete</p>
          <p>About 30â€“45 minutes on macOS with decent bandwidth and a 16 GB+ machine.</p>
        </div>
      </section>

      <!-- PREREQUISITES -->
      <section class="card">
        <h2>1. Prerequisitesï¼ˆå‰ç½®å‡†å¤‡ï¼‰</h2>
        <ul>
          <li>
            <strong>Hardwareï¼š</strong>16 GB RAM æ¨èç»™ 20B æ¨¡å‹ï¼ˆä¾‹å¦‚ gpt-oss-20Bï¼‰ã€‚7B / 8B æ¨¡å‹å¯ä»¥åœ¨æ›´å°æœºå™¨ä¸Šè·‘ã€‚
          </li>
          <li><strong>OSï¼š</strong>macOS / Linux / Windows éƒ½å¯ä»¥ã€‚ä¸‹é¢ç¤ºä¾‹ä½¿ç”¨ macOSã€‚</li>
          <li>
            <strong>Toolsï¼š</strong>
            <ul>
              <li>Homebrewï¼ˆmacOS å¥—ä»¶ç®¡ç†ï¼‰</li>
              <li>Node.js 18+ï¼ˆå†…å»º <code>fetch</code>ï¼‰</li>
              <li>
                Cloudflare å¸å· + ä¸€ä¸ªä½ çš„ç½‘åŸŸï¼ŒDNS å·²ç»æ‰˜ç®¡åˆ° Cloudflareï¼ˆnameserver æŒ‡è¿‡å»å³å¯ï¼‰ã€‚
              </li>
            </ul>
          </li>
        </ul>
      </section>

      <!-- LLAMA.CPP -->
      <section class="card">
        <h2>2. Run llama.cpp locallyï¼ˆåœ¨æœ¬æœºè·‘ LLM æœåŠ¡å™¨ï¼‰</h2>
        <p class="note">
          æˆ‘ä»¬ç”¨ <code>llama-server</code> è·‘ä¸€ä¸ª OpenAI å…¼å®¹çš„ HTTP æœåŠ¡å™¨ï¼Œè®©ä½ å¯ä»¥ç”¨
          <code>/v1/chat/completions</code> è°ƒæ¨¡å‹ã€‚
        </p>
        <pre><code># 2.1 å®‰è£… llama.cppï¼ˆmacOSï¼‰
brew install llama.cpp

# 2.2 å¯åŠ¨ llama-server
llama-server \
  -hf unsloth/gpt-oss-20b-GGUF:gpt-oss-20b-Q4_K_M.gguf \
  --port 5857 \
  --ctx-size 16384 \
  --threads -1 \
  --jinja \
  --reasoning-format none</code></pre>
        <p class="note">
          ç¬¬ä¸€æ¬¡è·‘ä¼šä» Hugging Face ä¸‹è½½ GGUF æ¨¡å‹ï¼ˆéœ€è¦ä¸€ç‚¹æ—¶é—´ï¼‰ã€‚ä¹‹åå°±ç›´æ¥ç”¨æœ¬åœ°å¿«å–ã€‚<br />
          å‚æ•°ç®€å•ç†è§£ä¸€ä¸‹ï¼š<code>--port</code> æ˜¯ HTTP ç«¯å£ã€<code>--ctx-size</code> æ˜¯æœ€å¤§ä¸Šä¸‹æ–‡ tokenã€
          <code>--threads</code> ä½¿ç”¨ CPU çº¿ç¨‹æ•°ã€‚
        </p>
        <p class="note">ç”¨ <code>curl</code> åšä¸€ä¸ªæœ¬æœºæµ‹è¯•ï¼ˆç¡®è®¤æ¨¡å‹æ­£å¸¸å·¥ä½œï¼‰ï¼š</p>
        <pre><code>curl http://localhost:5857/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-oss-20b",
    "messages": [
      { "role": "user", "content": "Hello from localhost 5857" }
    ]
  }'</code></pre>
        <p class="note">
          å¦‚æœä½ çœ‹åˆ°ç±»ä¼¼ OpenAI é£æ ¼çš„ JSON å›åº”ï¼Œé‚£è¿™ä¸€å±‚å°± OK äº†ã€‚<br />åˆ°ç›®å‰ä¸ºæ­¢ï¼Œä¸€åˆ‡éƒ½è¿˜åªåœ¨ä½ å®¶ç”µè„‘é‡Œé¢ï¼Œæ²¡æœ‰å¯¹å¤–ã€‚
        </p>
      </section>

      <!-- NODE GATEWAY -->
      <section class="card">
        <h2>3. Build a Node.js API-key gatewayï¼ˆåšä¸€ä¸ªè‡ªå·±çš„ API Gatewayï¼‰</h2>
        <p class="note">
          è¿™ä¸€æ­¥æ˜¯æ•´ä¸ªç³»ç»Ÿçš„ã€Œå¤§è„‘å…¥å£ã€ï¼š<br />
          <strong>æ‰€æœ‰å¤–éƒ¨è¯·æ±‚éƒ½å…ˆç»è¿‡ Gateway â†’ æ£€æŸ¥ API Key â†’ é€šè¿‡æ‰è½¬å‘ç»™ llama-serverã€‚</strong><br />
          åŒæ—¶åœ¨è¿™é‡Œç»Ÿä¸€å¤„ç† CORSï¼Œè®©æµè§ˆå™¨ä¸ä¼šè¢« CORS å¡ä½ã€‚
        </p>

        <h3>3.1 å»ºç«‹é¡¹ç›®ç›®å½• & å®‰è£…ä¾èµ–</h3>
        <pre><code>mkdir llm-gateway
cd llm-gateway

npm init -y
npm install express dotenv</code></pre>

        <p class="note">åœ¨ <code>package.json</code> ä¸­åŠ ä¸Š <code>"type": "module"</code> ä»¥åŠå¯åŠ¨æŒ‡ä»¤ï¼š</p>
        <pre><code>{
  "name": "llm-gateway",
  "version": "1.0.0",
  "main": "gateway.js",
  "type": "module",
  "scripts": {
    "start": "node gateway.js"
  },
  "dependencies": {
    "dotenv": "^16.4.0",
    "express": "^4.21.0"
  }
}</code></pre>

        <h3>3.2 .envï¼šé›†ä¸­é…ç½® API Keys / ä¸Šæ¸¸åœ°å€ / CORS</h3>
        <pre><code># .env
LLM_API_KEYS=sk-home-2025-1,sk-home-2025-2
LLM_UPSTREAM=http://127.0.0.1:5857
GATEWAY_PORT=8787
CORS_ORIGIN=http://localhost:7788</code></pre>
        <p class="note">
          è¯´æ˜ï¼š
          <ul>
            <li><code>LLM_API_KEYS</code>ï¼šå…è®¸çš„ API Keyï¼ˆå¯å¤šä¸ªï¼Œç”¨é€—å·éš”å¼€ï¼‰ã€‚</li>
            <li><code>LLM_UPSTREAM</code>ï¼šä½ çš„ <code>llama-server</code> åœ°å€ã€‚</li>
            <li><code>GATEWAY_PORT</code>ï¼šGateway å¯¹å¤–ç›‘å¬çš„æœ¬æœºç«¯å£ã€‚</li>
            <li><code>CORS_ORIGIN</code>ï¼šå‰ç«¯ç½‘é¡µæ‰€åœ¨çš„ Originï¼ˆå¼€å‘æ—¶æ˜¯ <code>http://localhost:7788</code>ï¼‰ã€‚</li>
          </ul>
        </p>

        <h3>3.3 gateway.jsï¼šAPI Key éªŒè¯ + CORS + è½¬å‘</h3>
        <p class="note">è¿™æ˜¯æ ¸å¿ƒé€»è¾‘ï¼Œé€»è¾‘å°½é‡å†™å¾—ç®€å•ç›´æ¥ï¼Œæ–¹ä¾¿åˆå­¦è€…é˜…è¯»ã€‚</p>
        <pre><code>// gateway.js
import express from "express";
import dotenv from "dotenv";

dotenv.config();

const app = express();

const PORT = parseInt(process.env.GATEWAY_PORT || "8787", 10);
const UPSTREAM_BASE = process.env.LLM_UPSTREAM || "http://127.0.0.1:5857";
const CORS_ORIGIN = process.env.CORS_ORIGIN || "http://localhost:7788";

// è§£æ API Keys åˆ° Set
const rawKeys = (process.env.LLM_API_KEYS || "").split(",");
const VALID_KEYS = new Set(
  rawKeys.map(k =&gt; k.trim()).filter(k =&gt; k.length &gt; 0)
);

console.log("âœ… LLM Gateway config:");
console.log("  - Port:", PORT);
console.log("  - Upstream:", UPSTREAM_BASE);
console.log("  - Valid API keys:", VALID_KEYS.size);

// --- å…¨å±€ CORS ä¸­é—´ä»¶ ---
app.use((req, res, next) =&gt; {
  res.header("Access-Control-Allow-Origin", CORS_ORIGIN);
  res.header(
    "Access-Control-Allow-Headers",
    "Origin, X-Requested-With, Content-Type, Accept, Authorization"
  );
  res.header("Access-Control-Allow-Methods", "GET, POST, OPTIONS");

  if (req.method === "OPTIONS") {
    return res.sendStatus(204); // CORS é¢„æ£€ï¼Œç›´æ¥æ”¾è¡Œ
  }
  next();
});

// è§£æ JSON body
app.use(express.json({ limit: "10mb" }));

// --- API Key éªŒè¯ä¸­é—´ä»¶ï¼ˆOpenAI é£æ ¼ï¼‰---
function authMiddleware(req, res, next) {
  const authHeader = req.headers["authorization"];
  if (!authHeader) {
    return res.status(401).json({
      error: {
        message: "Missing Authorization header. Use: Authorization: Bearer sk-xxx",
        type: "invalid_api_key",
      },
    });
  }

  const parts = authHeader.split(" ");
  if (parts.length !== 2 || parts[0].toLowerCase() !== "bearer") {
    return res.status(401).json({
      error: {
        message: "Invalid Authorization header format. Expected: Bearer sk-xxx",
        type: "invalid_api_key",
      },
    });
  }

  const token = parts[1].trim();
  if (!VALID_KEYS.has(token)) {
    console.warn("âŒ Invalid API key:", token);
    return res.status(401).json({
      error: {
        message: "Incorrect API key provided.",
        type: "invalid_api_key",
      },
    });
  }

  req.apiKey = token; // å¯ç”¨äº logging
  next();
}

// Health checkï¼ˆå¯é€‰ï¼‰
app.get("/health", (req, res) =&gt; {
  res.json({
    ok: true,
    upstream: UPSTREAM_BASE,
    keysConfigured: VALID_KEYS.size,
  });
});

// Chat Completions ä»£ç†
app.post("/v1/chat/completions", authMiddleware, async (req, res) =&gt; {
  try {
    const upstreamUrl = `${UPSTREAM_BASE}/v1/chat/completions`;
    console.log("â¡ï¸  /v1/chat/completions via key:", req.apiKey);

    const upstreamRes = await fetch(upstreamUrl, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
      },
      body: JSON.stringify(req.body),
    });

    res.status(upstreamRes.status);

    // å¤åˆ¶ä¸Šæ¸¸ headersï¼Œä½†ä¸è¦è¦†ç›–æˆ‘ä»¬çš„ CORS
    for (const [key, value] of upstreamRes.headers.entries()) {
      const lowerKey = key.toLowerCase();
      if (lowerKey === "transfer-encoding") continue;
      if (lowerKey.startsWith("access-control-")) continue;
      res.setHeader(key, value);
    }

    // å†è¡¥ä¸€æ¬¡ CORSï¼ˆä¿é™©ï¼‰
    res.setHeader("Access-Control-Allow-Origin", CORS_ORIGIN);
    res.setHeader(
      "Access-Control-Allow-Headers",
      "Origin, X-Requested-With, Content-Type, Accept, Authorization"
    );
    res.setHeader("Access-Control-Allow-Methods", "GET, POST, OPTIONS");

    if (upstreamRes.body) {
      upstreamRes.body.pipe(res); // æ”¯æŒ streaming
    } else {
      res.end();
    }
  } catch (err) {
    console.error("Gateway error:", err);
    res.status(500).json({
      error: {
        message: "Gateway failed to reach llama-server.",
        type: "gateway_error",
      },
    });
  }
});

app.listen(PORT, () =&gt; {
  console.log(`ğŸš€ LLM API Gateway listening on http://localhost:${PORT}`);
});</code></pre>

        <p class="note">
          æµ‹è¯•ï¼ˆæœ¬æœºï¼‰ï¼š<br />
          1ï¼‰å…ˆç¡®è®¤ <code>llama-server</code> åœ¨ 5857 è·‘ç€ï¼›<br />
          2ï¼‰åœ¨ <code>llm-gateway</code> ç›®å½•æ‰§è¡Œ <code>npm start</code>ï¼›<br />
          3ï¼‰è¯•ç€æ‰“ï¼š
        </p>
        <pre><code>curl http://localhost:8787/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-home-2025-1" \
  -d '{
    "model": "gpt-oss-20b",
    "messages": [
      { "role": "user", "content": "Hello via Node.js gateway" }
    ]
  }'</code></pre>
      </section>

      <!-- CLOUDFLARE TUNNEL -->
      <section class="card">
        <h2>4. Expose it with Cloudflare Tunnelï¼ˆç”¨ Cloudflare å®‰å…¨å¯¹å¤–å¼€æ”¾ï¼‰</h2>
        <p class="note">
          ç°åœ¨æˆ‘ä»¬è®©å¤–é¢çš„ä¸–ç•Œå¯ä»¥é€šè¿‡ <code>https://api.your-domain.com</code> è®¿é—®ä½ çš„ Gatewayï¼Œä½†ä¸éœ€è¦å¼€è·¯ç”±å™¨ç«¯å£ã€‚
        </p>

        <h3>4.1 å®‰è£…ä¸ç™»å½•</h3>
        <pre><code>brew install cloudflared

cloudflared tunnel login
# æµè§ˆå™¨ä¼šæ‰“å¼€ Cloudflare é¡µé¢ï¼Œç™»å½•å¹¶æˆæƒ</code></pre>

        <h3>4.2 åˆ›å»º Tunnel å’Œ DNS è®°å½•</h3>
        <pre><code># åˆ›å»ºä¸€ä¸ª Tunnelï¼ˆåå­—å¯ä»¥è‡ªå®šä¹‰ï¼‰
cloudflared tunnel create my-llm

# æŠŠå­åŸŸåæ˜ å°„åˆ°è¿™ä¸ª Tunnelï¼ˆä¾‹å¦‚ api.your-domain.comï¼‰
cloudflared tunnel route dns my-llm api.your-domain.com</code></pre>

        <h3>4.3 é…ç½® ~/.cloudflared/config.yml</h3>
        <pre><code>tunnel: &lt;YOUR-TUNNEL-UUID&gt;
credentials-file: /Users/&lt;you&gt;/.cloudflared/&lt;YOUR-TUNNEL-UUID&gt;.json

ingress:
  - hostname: api.your-domain.com
    service: http://localhost:8787   # æŒ‡å‘ Node.js Gateway
  - service: http_status:404         # å…œåº•è§„åˆ™</code></pre>

        <h3>4.4 å¯åŠ¨ Tunnel</h3>
        <pre><code>cloudflared tunnel run my-llm</code></pre>

        <p class="note">
          ç°åœ¨ä½ å¯ä»¥ä»ä»»ä½•åœ°æ–¹æµ‹è¯•ï¼ˆåªè¦ DNS åˆ·æ–°å®Œæˆï¼‰ï¼š
        </p>
        <pre><code>curl https://api.your-domain.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-home-2025-1" \
  -d '{
    "model": "gpt-oss-20b",
    "messages": [
      { "role": "user", "content": "Hello from the internet" }
    ]
  }'</code></pre>
      </section>

      <!-- CHAT UI -->
      <section class="card">
        <h2>5. Simple chat UIï¼ˆå‰ç«¯èŠå¤©é¡µé¢ï¼‰</h2>
        <p class="note">
          æœ€åï¼Œæˆ‘ä»¬åšä¸€ä¸ªç®€å•çš„ <code>index.html</code>ï¼Œåœ¨æµè§ˆå™¨ç›´æ¥è·Ÿä½ çš„ã€Œå®¶åº­ç‰ˆ OpenAI APIã€èŠå¤©ã€‚è¿™é‡Œåªç»™å…³é”®éƒ¨åˆ†ï¼š
        </p>

        <h3>5.1 ç”¨é™æ€æœåŠ¡å™¨è·‘å‰ç«¯</h3>
        <pre><code># åœ¨å‰ç«¯ç›®å½•ï¼ˆåŒ…å« index.html çš„åœ°æ–¹ï¼‰
python3 -m http.server 7788

# æµè§ˆå™¨æ‰“å¼€
http://localhost:7788</code></pre>

        <h3>5.2 Chat UI å†…éƒ¨çš„å…³é”® JS ç‰‡æ®µ</h3>
        <pre><code>&lt;script&gt;
const API_URL = "https://api.your-domain.com/v1/chat/completions";
const API_KEY = "sk-home-2025-1"; // ä¸è¦ç»™åˆ«äººçœ‹ï¼Œç”Ÿäº§ç¯å¢ƒè¯·æ”¹ç”¨åç«¯æ³¨å…¥

async function sendMessage(userText) {
  const body = {
    model: "gpt-oss-20b",
    messages: [
      { role: "user", content: userText }
    ],
    stream: false
  };

  const res = await fetch(API_URL, {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      "Authorization": "Bearer " + API_KEY
    },
    body: JSON.stringify(body)
  });

  if (!res.ok) {
    throw new Error("HTTP " + res.status);
  }
  const json = await res.json();
  const reply = json.choices?.[0]?.message?.content ?? "(no content)";
  return reply;
}
&lt;/script&gt;</code></pre>

        <p class="note">
          åœ¨çœŸæ­£ç»™åˆ«äººç”¨çš„æ—¶å€™ï¼Œä¸å»ºè®®æŠŠ API Key ç›´æ¥å†™æ­»åœ¨å‰ç«¯ã€‚<br />
          è¿™ç¯‡æ–‡ç« ä¸»è¦æ˜¯æ•™å­¦ï¼šè®©ä½ å…ˆè·‘å¾—èµ·æ¥ï¼Œä¹‹åå¯ä»¥å†å‡çº§ã€Œè´¦å·ç³»ç»Ÿã€å’Œã€ŒçœŸæ­£çš„ Key ç®¡ç†ã€ã€‚
        </p>
      </section>

      <!-- CORS & COMMON ERRORS -->
      <section class="card">
        <h2>6. CORS & common errorsï¼ˆå¸¸è§é—®é¢˜æ’é”™ï¼‰</h2>

        <h3>6.1 favicon.ico 404</h3>
        <pre><code>GET http://localhost:7788/favicon.ico 404 (File not found)</code></pre>
        <p class="note">
          è¿™åªæ˜¯æµè§ˆå™¨è‡ªåŠ¨è¯·æ±‚ç½‘ç«™å›¾ç¤ºï¼Œæ‰¾ä¸åˆ°å°± 404ï¼Œå¯¹åŠŸèƒ½æ²¡æœ‰ä»»ä½•å½±å“ã€‚ä½ å¯ä»¥ï¼š
        </p>
        <ul>
          <li>ç›´æ¥å¿½ç•¥ï¼›</li>
          <li>æˆ–åœ¨ <code>&lt;head&gt;</code> é‡ŒåŠ ï¼š<code>&lt;link rel="icon" href="data:,"&gt;</code>ã€‚</li>
        </ul>

        <h3>6.2 CORS è¢«æŒ¡ï¼ˆæ ¸å¿ƒé—®é¢˜ï¼‰</h3>
        <pre><code>Access to fetch at 'https://api.your-domain.com/v1/chat/completions'
from origin 'http://localhost:7788' has been blocked by CORS policy:
The 'Access-Control-Allow-Origin' header contains the invalid value ''.</code></pre>

        <p class="note">
          å‡ºç°è¿™ç±»é”™è¯¯ï¼ŒåŸºæœ¬ä¸Šéƒ½æ˜¯ï¼š
          <strong>Gateway å›ç»™æµè§ˆå™¨çš„ CORS å¤´æ²¡è®¾å¥½ï¼Œæˆ–è¢«ä¸Šæ¸¸è¦†ç›–æ‰ã€‚</strong><br />
          è®°å¾—ä¸¤ä»¶äº‹ï¼š
        </p>
        <ul>
          <li>åœ¨ Gateway é‡Œç»Ÿä¸€è®¾ç½® <code>Access-Control-Allow-Origin</code>ï¼ˆç”¨ <code>CORS_ORIGIN</code>ï¼‰ã€‚</li>
          <li>ä» <code>llama-server</code> è½¬å‘ header æ—¶ï¼Œä¸è¦è½¬ä¸Šæ¸¸çš„ <code>access-control-*</code>ï¼Œé¿å…è¦†ç›–ã€‚</li>
        </ul>
        <p class="note">
          å¦‚æœä½ ç…§ç€æœ¬æ–‡çš„ <code>gateway.js</code> å®ä½œï¼ŒåŸºæœ¬ä¸Šå°±ä¸ä¼šå†è¢« CORS å¡ä½ã€‚
        </p>
      </section>

      <!-- SECURITY / NEXT -->
      <section class="card">
        <h2>7. Security & next stepsï¼ˆå®‰å…¨ä¸ä¸‹ä¸€æ­¥ï¼‰</h2>
        <ul>
          <li>æŠŠ <code>llama-server</code> åªç»‘å®šåœ¨ <code>localhost</code>ï¼Œä¸è¦å¯¹å¤–å¼€æ”¾ç«¯å£ã€‚</li>
          <li>
            æ‰€æœ‰æ¥è‡ªå…¬ç½‘çš„æµé‡éƒ½å¿…é¡»ç»ç”±ï¼š
            <code>Cloudflare â†’ Tunnel â†’ Node.js Gatewayï¼ˆAPI Key éªŒè¯ï¼‰</code>ã€‚
          </li>
          <li>é¿å…åœ¨å…¬å¼€å‰ç«¯ç¡¬ç¼–ç çœŸå® API Keyï¼›æœ¬ç¯‡ç¤ºä¾‹åå‘æ•™å­¦ / è‡ªç”¨ã€‚</li>
          <li>å¯ä»¥åœ¨ Gateway åŠ å…¥ç®€å•çš„ rate limitï¼Œé˜²æ­¢ Key è¢«æ»¥ç”¨åˆ·çˆ†ä½ çš„æœºå™¨ã€‚</li>
          <li>æœ‰éœ€è¦çš„è¯ï¼Œå†å¾€ã€Œå¤šæ¨¡å‹è·¯ç”±ã€å’Œã€ŒRAGï¼ˆæ£€ç´¢å¢å¼ºï¼‰ã€æ–¹å‘æ‰©å±•ã€‚</li>
        </ul>
        <p class="note">
          åˆ°è¿™é‡Œï¼Œä½ å°±å·²ç»æ‹¥æœ‰ä¸€ä¸ªã€Œå®¶åº­ç‰ˆ OpenAI APIã€ï¼š<br />
          æ¨¡å‹è·‘åœ¨ä½ è‡ªå·±çš„æœºå™¨ä¸Šï¼ŒAPI é£æ ¼å’Œ OpenAI éå¸¸æ¥è¿‘ï¼Œè€Œä¸”ä½ å®Œå…¨æ§åˆ¶ Key å’Œè®¿é—®æƒé™ã€‚
        </p>
      </section>

      <footer>
        å†™ç»™æœªæ¥çš„è‡ªå·±å’Œå›¢é˜Ÿé‡Œæ¯”è¾ƒ junior çš„åŒäº‹ï¼š<br />
        å½“ä½ å¿˜è®°æ€ä¹ˆæŠŠå®¶é‡Œçš„æœºå™¨å˜æˆä¸€ä¸ªå°å‹ OpenAI æ—¶ï¼Œå›æ¥çœ‹è¿™ä¸€ç¯‡ï¼Œä» Step 1 è·Ÿåˆ° Step 7ï¼Œå°±å¯ä»¥é‡æ–°è·‘èµ·æ•´ä¸ªç³»ç»Ÿã€‚
      </footer>
    </div>
  </body>
</html>
