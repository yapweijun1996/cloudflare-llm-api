<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Home LLM + Cloudflare Tunnel Guide</title>
    <style>
      :root {
        color-scheme: light;
      }
      * {
        box-sizing: border-box;
      }
      body {
        margin: 0;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", system-ui, sans-serif;
        background: #f7f9fc;
        color: #0f172a;
        line-height: 1.6;
      }
      .page {
        max-width: 1100px;
        margin: 0 auto;
        padding: 32px 18px 60px;
      }
      header {
        margin-bottom: 28px;
      }
      .eyebrow {
        letter-spacing: 0.2em;
        text-transform: uppercase;
        color: #64748b;
        font-size: 12px;
        margin: 0 0 10px;
      }
      h1 {
        font-size: clamp(28px, 4vw, 40px);
        margin: 0 0 14px;
      }
      p.lead {
        margin: 0 0 14px;
        color: #334155;
      }
      .chips {
        display: flex;
        gap: 8px;
        flex-wrap: wrap;
        margin-top: 10px;
      }
      .chip {
        padding: 6px 10px;
        border: 1px solid #e2e8f0;
        border-radius: 999px;
        background: #fff;
        font-size: 13px;
        color: #334155;
      }
      section {
        margin-bottom: 26px;
      }
      .grid {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(230px, 1fr));
        gap: 12px;
      }
      .card {
        background: #fff;
        border: 1px solid #e2e8f0;
        border-radius: 12px;
        padding: 14px;
        box-shadow: 0 8px 24px rgba(15, 23, 42, 0.04);
      }
      .card h3,
      .card h2 {
        margin-top: 0;
        margin-bottom: 8px;
      }
      h2 {
        margin: 0 0 8px;
        font-size: 22px;
      }
      h3 {
        margin: 0 0 6px;
        font-size: 18px;
      }
      ul {
        padding-left: 18px;
        margin: 8px 0 0;
      }
      li {
        margin: 4px 0;
      }
      pre {
        background: #0f172a;
        color: #e2e8f0;
        padding: 14px;
        border-radius: 10px;
        overflow-x: auto;
        font-size: 13px;
        border: 1px solid #1e293b;
        margin: 12px 0 0;
      }
      code {
        font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
      }
      footer {
        border-top: 1px solid #e2e8f0;
        padding-top: 14px;
        margin-top: 32px;
        color: #64748b;
        font-size: 14px;
      }
      .note {
        color: #334155;
      }
    </style>
  </head>
  <body>
    <div class="page">
      <header>
        <p class="eyebrow">Hands-on Guide</p>
        <h1>Host your own OpenAI-style API at home with llama.cpp + Cloudflare Tunnel</h1>
        <p class="lead">
          This blog walks a junior teammate through running a local GGUF model with llama.cpp, exposing it safely through
          Cloudflare Tunnel, protecting it with a Node.js API-key gateway, and using the simple chat UI in
          <code>index.html</code>.
        </p>
        <div class="chips">
          <span class="chip">llama.cpp</span>
          <span class="chip">Cloudflare Tunnel</span>
          <span class="chip">Node.js Gateway</span>
          <span class="chip">Chat UI</span>
        </div>
      </header>

      <section class="grid">
        <div class="card">
          <p class="eyebrow" style="letter-spacing: 0.08em">What you get</p>
          <p>An OpenAI-compatible <code>/v1/chat/completions</code> API on your own hardware with API keys.</p>
        </div>
        <div class="card">
          <p class="eyebrow" style="letter-spacing: 0.08em">Who this is for</p>
          <p>Juniors who want a copy-paste friendly path to a self-hosted LLM with sensible defaults.</p>
        </div>
        <div class="card">
          <p class="eyebrow" style="letter-spacing: 0.08em">Time to complete</p>
          <p>About 30-45 minutes on macOS with decent bandwidth.</p>
        </div>
      </section>

      <section class="card">
        <h2>0. Architecture</h2>
        <p class="note">
          Requests flow from the browser to your gateway, then to llama.cpp, with Cloudflare handling the public entry
          point. API keys are enforced at the gateway.
        </p>
        <pre><code>[Browser / App] -- HTTPS + Authorization: Bearer sk-xxx
        |
Cloudflare Edge (DNS/TLS/WAF)
        |
cloudflared tunnel
        |
Node.js Gateway @ localhost:8787
        |
llama-server (llama.cpp) @ localhost:5857</code></pre>
      </section>

      <section class="card">
        <h3>1. Prerequisites</h3>
        <ul>
          <li>Hardware: 16 GB RAM for gpt-oss-20B; smaller 7B/8B models work on leaner boxes.</li>
          <li>OS: macOS/Linux/Windows; examples use macOS.</li>
          <li>Tools: Homebrew (mac), Node.js 18+, a Cloudflare account with a domain managed there.</li>
        </ul>
      </section>

      <section class="card">
        <h3>2. Run llama.cpp locally</h3>
        <p class="note">Install once, then start the OpenAI-compatible server.</p>
        <pre><code>brew install llama.cpp

# Launch the server (downloads model on first run)
llama-server \
  -hf unsloth/gpt-oss-20b-GGUF:gpt-oss-20b-Q4_K_M.gguf \
  --port 5857 \
  --ctx-size 16384 \
  --threads -1 \
  --jinja \
  --reasoning-format none</code></pre>
        <p class="note">Sanity check the local API:</p>
        <pre><code>curl http://localhost:5857/v1/chat/completions \
  -H "Content-Type": "application/json" \
  -d '{ "model": "gpt-oss-20b", "messages": [{ "role": "user", "content": "Hello from localhost" }] }'</code></pre>
      </section>

      <section class="card">
        <h3>3. Add a Node.js API-key gateway</h3>
        <p class="note">
          The gateway enforces Bearer tokens and fixes CORS for browsers. Files live in <code>gateway/</code>.
        </p>
        <pre><code>cd gateway
cp .env.example .env
# Edit .env to match your host and keys:
# LLM_API_KEYS=sk-tno-llm-2025-1,sk-tno-llm-2025-2
# LLM_UPSTREAM=http://127.0.0.1:5857
# GATEWAY_PORT=8787
# CORS_ALLOW_ORIGIN=http://localhost:7788

npm install   # first time
npm start     # runs gateway.js on port 8787</code></pre>
        <p class="note">
          In <code>gateway.js</code>, <code>authMiddleware</code> checks <code>Authorization: Bearer sk-xxx</code>, and
          CORS headers are applied globally plus after proxying upstream to avoid blank upstream CORS. This stops browser
          CORS errors.
        </p>
      </section>

      <section class="card">
        <h3>4. Expose safely with Cloudflare Tunnel</h3>
        <p class="note">Map a public HTTPS hostname to your local gateway.</p>
        <pre><code>brew install cloudflared
cloudflared tunnel login      # one-time auth in browser
cloudflared tunnel create llm-home
cloudflared tunnel route dns llm-home api.your-domain.com

# config.yaml example (in ~/.cloudflared/)
url: http://localhost:8787
tunnel: llm-home
credentials-file: /Users/you/.cloudflared/llm-home.json

cloudflared tunnel run llm-home</code></pre>
        <p class="note">
          Now <code>https://api.your-domain.com/v1/chat/completions</code> reaches your gateway, which checks the API key
          and forwards to llama.cpp on localhost.
        </p>
      </section>

      <section class="card">
        <h3>5. Use the chat UI</h3>
        <p class="note">
          <code>index.html</code> is a streaming chatbox showing first-token latency and tokens-per-second.
        </p>
        <pre><code># Serve the frontend (from repo root)
python3 -m http.server 7788

# In index.html (already set)
const API_URL = "http://localhost:8787/v1/chat/completions";
const API_KEY = "sk-tno-llm-2025-1";  // replace with your key</code></pre>
        <ul>
          <li>Enter your prompt; the assistant streams via Server-Sent Events.</li>
          <li>Metric cards show first-token time and approximate tokens per second.</li>
          <li>CORS is handled by the gateway; the page uses plain HTML/CSS.</li>
        </ul>
      </section>

      <section class="card">
        <h3>6. Deploy notes</h3>
        <ul>
          <li>Change <code>CORS_ALLOW_ORIGIN</code> to your production frontend host.</li>
          <li>Rotate <code>LLM_API_KEYS</code> regularly; keep them out of the browser.</li>
          <li>Put the gateway behind Cloudflare WAF/threat rules; keep llama-server bound to localhost.</li>
          <li>For HTTPS locally, front the gateway with a reverse proxy (optional if using tunnel).</li>
        </ul>
      </section>

      <footer>
        Made for teammates who need a clear, copy-paste path to a home-hosted LLM. Adjust model and CORS settings to fit
        your hardware and domain.
      </footer>
    </div>
  </body>
</html>
